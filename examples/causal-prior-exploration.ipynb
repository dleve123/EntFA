{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/entfa/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entitites:\n",
      "<class 'list'>\n",
      "ent text:\n",
      "<class 'list'>\n",
      "['Sydney', 'first', 'Waverley', 'two', 'Australian']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "source = 'The city was brought to a standstill on 15 December last year when a gunman held 18 hostages for 17 hours. Family members of victims Tori Johnson and Katrina Dawson were in attendance. Images of the floral tributes that filled the city centre in the wake of the siege were projected on to the cafe and surrounding buildings in an emotional twilight ceremony. Prime Minister Malcolm Turnbull gave an address saying a \"whole nation resolved to answer hatred with love\". \"Testament to the spirit of Australians is that with such unnecessary, thoughtless tragedy, an amazing birth of mateship, unity and love occurs. Proud to be Australian,\" he said. How the Sydney siege unfolded. New South Wales Premier Mike Baird has also announced plans for a permanent memorial to be built into the pavement in Martin Place. Clear cubes containing flowers will be embedded into the concrete and will shine with specialised lighting. It is a project inspired by the massive floral tributes that were left in the days after the siege. \"Something remarkable happened here. As a city we were drawn to Martin Place. We came in shock and in sorrow but every step we took was with purpose,\" he said on Tuesday.'\n",
    "prediction = 'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian city.'\n",
    "\n",
    "entities = nlp(prediction).to_json()['ents']\n",
    "ent_text = [prediction[e['start']: e['end']] for e in entities]\n",
    "\n",
    "print('entitites:')\n",
    "print(type(entities))\n",
    "\n",
    "print('ent text:')\n",
    "print(type(ent_text))\n",
    "print(ent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<mask>',\n",
       "  'Sydney has marked the <mask>',\n",
       "  'Sydney has marked the first anniversary of the siege at the <mask>',\n",
       "  'Sydney has marked the first anniversary of the siege at the Waverley cafe in which <mask>',\n",
       "  'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the <mask>'],\n",
       " ['Sydney',\n",
       "  'Sydney has marked the first',\n",
       "  'Sydney has marked the first anniversary of the siege at the Waverley',\n",
       "  'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two',\n",
       "  'Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian'],\n",
       " [(0, 6), (22, 27), (60, 68), (83, 86), (124, 134)],\n",
       " ['Sydney', 'first', 'Waverley', 'two', 'Australian'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_clm_inputs(source, target, ent_parts=None):\n",
    "    \"\"\"For Masked Language Model. For BART only.\"\"\"\n",
    "    if ent_parts is None:\n",
    "        ent_parts = nlp(target).to_json()['ents']\n",
    "    \n",
    "    inputs, targets = [], []\n",
    "    positions, entities = [], []\n",
    "\n",
    "    for e in ent_parts:\n",
    "        inputs.append(target[0: e['start']] + '<mask>')\n",
    "        targets.append(target[:e['end']])\n",
    "        entities.append(target[e['start']: e['end']])\n",
    "        positions.append((e['start'], e['end']))\n",
    "    \n",
    "    return inputs, targets, positions, entities\n",
    "\n",
    "inputs = prepare_clm_inputs(source, prediction, ent_parts=entities)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sydney has marked the first anniversary of the siege at the <mask>\n",
      "Sydney has marked the first anniversary of the siege at the Waverley\n"
     ]
    }
   ],
   "source": [
    "masked_input = inputs[0][2]\n",
    "print(masked_input)\n",
    "target = inputs[1][2]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 50265])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = tokenizer(masked_input, return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = model(input_ids).logits\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized masked input shape: torch.Size([1, 16])\n",
      "token ID -> token:\n",
      "0 | '<s>'\n",
      "104 | 'S'\n",
      "9611 | 'yd'\n",
      "2596 | 'ney'\n",
      "34 | ' has'\n",
      "4760 | ' marked'\n",
      "5 | ' the'\n",
      "78 | ' first'\n",
      "4038 | ' anniversary'\n",
      "9 | ' of'\n",
      "5 | ' the'\n",
      "19951 | ' siege'\n",
      "23 | ' at'\n",
      "5 | ' the'\n",
      "50264 | '<mask>'\n",
      "2 | '</s>'\n"
     ]
    }
   ],
   "source": [
    "print('tokenized masked input shape:', input_ids.shape)\n",
    "\n",
    "print('token ID -> token:')\n",
    "\n",
    "for token_id in input_ids[0]:\n",
    "    print(f\"{token_id} | '{tokenizer.decode(token_id)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask position in input sequence: 14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_token_id</th>\n",
       "      <th>predicted_token</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4290</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>0.049893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059</td>\n",
       "      <td>Australian</td>\n",
       "      <td>0.039405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1082</td>\n",
       "      <td>site</td>\n",
       "      <td>0.027545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276</td>\n",
       "      <td>same</td>\n",
       "      <td>0.016303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3062</td>\n",
       "      <td>airport</td>\n",
       "      <td>0.014825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted_token_id predicted_token    values\n",
       "0                4290          Sydney  0.049893\n",
       "1                2059      Australian  0.039405\n",
       "2                1082            site  0.027545\n",
       "3                 276            same  0.016303\n",
       "4                3062         airport  0.014825"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "print('mask position in input sequence:', masked_index)\n",
    "\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "\n",
    "predicted_tokens = [token.replace(' ', '_') for token in tokenizer.decode(predictions).split()]\n",
    "pd.DataFrame({'predicted_token_id': predictions, 'predicted_token': predicted_tokens, 'values': values.detach().numpy(),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   104,  9611,  2596,    34,  4760,     5,    78,  4038,     9,\n",
       "             5, 19951,    23,     5,   305,  9903,   607,     2]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens = tokenizer.encode(target, return_tensors='pt')\n",
    "target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting target: Sydney has marked the first anniversary of the siege at the Waverley\n",
      "target tokens: tensor([[    0,   104,  9611,  2596,    34,  4760,     5,    78,  4038,     9,\n",
      "             5, 19951,    23,     5,   305,  9903,   607,     2]])\n",
      "[check] encoded-decoded target: <s>Sydney has marked the first anniversary of the siege at the Waverley</s>\n"
     ]
    }
   ],
   "source": [
    "target_tokens = tokenizer.encode(target, return_tensors='pt')\n",
    "target_encoded_decoded = tokenizer.decode(target_tokens[0])\n",
    "print('starting target:', target)\n",
    "print('target tokens:', target_tokens)\n",
    "print('[check] encoded-decoded target:', target_encoded_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   104,  9611,  2596,    34,  4760,     5,    78,  4038,     9,\n",
       "             5, 19951,    23,     5, 50264,     2]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   104,  9611,  2596,    34,  4760,     5,    78,  4038,     9,\n",
       "             5, 19951,    23,     5,   305,  9903,   607,     2]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50264)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0][len(input_ids[0]) - 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(305)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens[0][len(input_ids[0]) - 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aver'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(target_tokens[len(input_ids[0]) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[305, 9903, 352]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(' Waverly', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' W'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 104,\n",
       " 9611,\n",
       " 2596,\n",
       " 34,\n",
       " 4760,\n",
       " 5,\n",
       " 78,\n",
       " 4038,\n",
       " 9,\n",
       " 5,\n",
       " 19951,\n",
       " 23,\n",
       " 5,\n",
       " 305,\n",
       " 9903,\n",
       " 607,\n",
       " 16381,\n",
       " 11,\n",
       " 61,\n",
       " 80,\n",
       " 2]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked input: <mask>\n",
      "print target: Sydney\n",
      "len input ids: 3\n",
      "len of target tokens: 5\n",
      "len of the model output scores: 5\n",
      "forced output: ['</s><s>Sydney</s>']\n",
      "0 | '<s>' | 0.9999822378158569\n",
      "104 | 'S' | 4.914681994705461e-06\n",
      "9611 | 'yd' | 0.0021797779481858015\n",
      "2596 | 'ney' | 0.9180679321289062\n",
      "2 | '</s>' | 0.5756633877754211\n",
      "---------\n",
      "masked input: Sydney has marked the <mask>\n",
      "print target: Sydney has marked the first\n",
      "len input ids: 9\n",
      "len of target tokens: 9\n",
      "len of the model output scores: 9\n",
      "forced output: ['</s><s>Sydney has marked the first</s>']\n",
      "0 | '<s>' | 0.9999856948852539\n",
      "104 | 'S' | 0.9999973773956299\n",
      "9611 | 'yd' | 0.9999998807907104\n",
      "2596 | 'ney' | 0.9999958276748657\n",
      "34 | ' has' | 0.9999608993530273\n",
      "4760 | ' marked' | 0.9995445609092712\n",
      "5 | ' the' | 0.9999163150787354\n",
      "78 | ' first' | 0.013117797672748566\n",
      "2 | '</s>' | 0.003610642161220312\n",
      "---------\n",
      "masked input: Sydney has marked the first anniversary of the siege at the <mask>\n",
      "print target: Sydney has marked the first anniversary of the siege at the Waverley\n",
      "len input ids: 16\n",
      "len of target tokens: 18\n",
      "len of the model output scores: 18\n",
      "forced output: ['</s><s>Sydney has marked the first anniversary of the siege at the Waverley</s>']\n",
      "0 | '<s>' | 0.9999861717224121\n",
      "104 | 'S' | 0.999997615814209\n",
      "9611 | 'yd' | 1.0\n",
      "2596 | 'ney' | 0.9999980926513672\n",
      "34 | ' has' | 0.9999978542327881\n",
      "4760 | ' marked' | 0.9999996423721313\n",
      "5 | ' the' | 0.9999978542327881\n",
      "78 | ' first' | 0.9999939203262329\n",
      "4038 | ' anniversary' | 0.9999997615814209\n",
      "9 | ' of' | 0.9999915361404419\n",
      "5 | ' the' | 0.9999996423721313\n",
      "19951 | ' siege' | 0.9999791383743286\n",
      "23 | ' at' | 0.9999626874923706\n",
      "5 | ' the' | 0.9999064207077026\n",
      "305 | ' W' | 0.001133607467636466\n",
      "9903 | 'aver' | 0.08252919465303421\n",
      "607 | 'ley' | 0.9853810667991638\n",
      "2 | '</s>' | 0.0038105067797005177\n",
      "---------\n",
      "masked input: Sydney has marked the first anniversary of the siege at the Waverley cafe in which <mask>\n",
      "print target: Sydney has marked the first anniversary of the siege at the Waverley cafe in which two\n",
      "len input ids: 22\n",
      "len of target tokens: 22\n",
      "len of the model output scores: 19\n",
      "forced output: ['</s><s>Sydney has marked the first anniversary of the siege at the Waverley cafe</s>']\n",
      "0 | '<s>' | 0.9999918937683105\n",
      "104 | 'S' | 0.9999980926513672\n",
      "9611 | 'yd' | 1.0\n",
      "2596 | 'ney' | 0.999996542930603\n",
      "34 | ' has' | 0.9999980926513672\n",
      "4760 | ' marked' | 0.9999995231628418\n",
      "5 | ' the' | 0.9999982118606567\n",
      "78 | ' first' | 0.9999945163726807\n",
      "4038 | ' anniversary' | 0.9999998807907104\n",
      "9 | ' of' | 0.9999902248382568\n",
      "5 | ' the' | 0.9999998807907104\n",
      "19951 | ' siege' | 0.9999778270721436\n",
      "23 | ' at' | 0.9999521970748901\n",
      "5 | ' the' | 0.999995231628418\n",
      "305 | ' W' | 0.9999992847442627\n",
      "9903 | 'aver' | 0.9999992847442627\n",
      "607 | 'ley' | 0.9999948740005493\n",
      "16381 | ' cafe' | 0.9999910593032837\n",
      "11 | ' in' | 0.9999876022338867\n",
      "---------\n",
      "masked input: Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the <mask>\n",
      "print target: Sydney has marked the first anniversary of the siege at the Waverley cafe in which two women were killed by a gunman in the Australian\n",
      "len input ids: 31\n",
      "len of target tokens: 31\n",
      "len of the model output scores: 19\n",
      "forced output: ['</s><s>Sydney has marked the first anniversary of the siege at the Waverley cafe</s>']\n",
      "0 | '<s>' | 0.9999921321868896\n",
      "104 | 'S' | 0.9999961853027344\n",
      "9611 | 'yd' | 1.0\n",
      "2596 | 'ney' | 0.9999964237213135\n",
      "34 | ' has' | 0.9999980926513672\n",
      "4760 | ' marked' | 0.9999994039535522\n",
      "5 | ' the' | 0.9999974966049194\n",
      "78 | ' first' | 0.9999924898147583\n",
      "4038 | ' anniversary' | 0.9999997615814209\n",
      "9 | ' of' | 0.9999936819076538\n",
      "5 | ' the' | 0.9999997615814209\n",
      "19951 | ' siege' | 0.9999810457229614\n",
      "23 | ' at' | 0.9999970197677612\n",
      "5 | ' the' | 0.9999969005584717\n",
      "305 | ' W' | 0.9999991655349731\n",
      "9903 | 'aver' | 0.9999991655349731\n",
      "607 | 'ley' | 0.9999905824661255\n",
      "16381 | ' cafe' | 0.99998939037323\n",
      "11 | ' in' | 0.9999946355819702\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for named_entity_index in range(len(inputs[0])):\n",
    "    masked_input = inputs[0][named_entity_index]\n",
    "    print('masked input:', masked_input)\n",
    "    target = inputs[1][named_entity_index]\n",
    "    print('print target:', target)\n",
    "    target_tokens = tokenizer.encode(target, return_tensors='pt')\n",
    "\n",
    "    input_ids = tokenizer(masked_input, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    print('len input ids:', input_ids.shape[1])\n",
    "\n",
    "    def prefix_allowed_tokens_fn(_batch_id, input_ids):\n",
    "        current_step = len(input_ids) - 1\n",
    "        return target_tokens[0, current_step].tolist()\n",
    "        \n",
    "    model_output = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=1,\n",
    "        early_stopping=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "    )\n",
    "\n",
    "    print('len of target tokens:', len(target_tokens[0].tolist()))\n",
    "    print('len of the model output scores:', len(model_output.scores))\n",
    "    print('forced output:', tokenizer.batch_decode(model_output.sequences))\n",
    "\n",
    "    for target_token, token_score in zip(target_tokens[0].tolist(), model_output.scores):\n",
    "        target_parts = tokenizer.decode(target_token, add_special_tokens=False)\n",
    "        token_score = token_score.softmax(dim=1)[0, target_token]\n",
    "        print(f\"{target_token} | '{target_parts}' | {token_score}\")\n",
    "    \n",
    "    print('---------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s><s>Sydney</s>']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model_output.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob of Waverley: 9.218802666182273e-05\n"
     ]
    }
   ],
   "source": [
    "print('prob of Waverley:', 0.001133607467636466 * 0.08252919465303421 * 0.9853810667991638)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_output.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_output.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>Sydney has marked the first anniversary of the siege at the<mask></s>']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 104, 9611, 2596, 34, 4760, 5, 78, 4038, 9, 5, 19951, 23, 5, 4290, 5416, 607, 4679]\n"
     ]
    }
   ],
   "source": [
    "max_scores = [score.argmax().item() for score in model_output.scores]\n",
    "print(max_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Sydney has marked the first anniversary of the siege at the Sydneyadiley Bridge'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(max_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s><s>Sydney has marked the first anniversary of the siege at the Sydney Opera House.</s>']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model_output.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50264"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with `<mask>`\n",
    "\n",
    "Prefixing `<mask>` with whitespace doesn't impact tokenization, it seems.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sydney has marked the first anniversary of the siege at the <mask>\n",
      "[104, 9611, 2596, 34, 4760, 5, 78, 4038, 9, 5, 19951, 23, 5, 50264]\n",
      "--Sydney has marked the first anniversary of the siege at the<mask>--\n"
     ]
    }
   ],
   "source": [
    "foo = \"Sydney has marked the first anniversary of the siege at the <mask>\"\n",
    "encoded = tokenizer.encode(foo, add_special_tokens=False)\n",
    "print(foo)\n",
    "print(encoded)\n",
    "print(f\"--{tokenizer.decode(encoded, clean_up_tokenization_spaces=True)}--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sydney has marked the first anniversary of the siege at the<mask>\n",
      "[104, 9611, 2596, 34, 4760, 5, 78, 4038, 9, 5, 19951, 23, 5, 50264]\n",
      "--Sydney has marked the first anniversary of the siege at the<mask>--\n"
     ]
    }
   ],
   "source": [
    "foo = \"Sydney has marked the first anniversary of the siege at the<mask>\"\n",
    "encoded = tokenizer.encode(foo, add_special_tokens=False)\n",
    "print(foo)\n",
    "print(encoded)\n",
    "print(f\"--{tokenizer.decode(encoded, clean_up_tokenization_spaces=True)}--\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "459b9217564289376ecf29bb4b901d4606aa3f1a906409f5398bc9c0e418da4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('entfa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
